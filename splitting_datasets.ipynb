{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Datasets\n",
    "In supervised training, you will want to split your training set into training and evaluation samples, and only use the test set for the final model test.\n",
    "To achieve that we zip a dataset with an index range, filter by a certain threshold and then map the dataset back to get rid of that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to rerun and should close the session, if one is open.\n",
    "try: \n",
    "    sess.close()\n",
    "except NameError:\n",
    "    print(\"Don't worry. Need to ignore this error once\")\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "We have 20 samples with their labels and want to split at 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_img = np.array([[0,0],[1,1],[2,2],[3,3],[4,4],[5,5],[6,6],[7,7],[8,8],[9,9],\n",
    "                  [10,0],[11,1],[12,2],[13,3],[14,4],[15,5],[16,6],[17,7],[18,8],[19,9]])\n",
    "tr_lbl = np.array([0,3,2,0,2,1,2,3,0,3,0,2,1,3,1,0,3,2,3,3])\n",
    "N = 20\n",
    "ratio = 0.2\n",
    "idx = np.array(range(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the indexed datasets from the numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_img_tensor = tf.constant(tr_img)\n",
    "tr_lbl_tensor = tf.constant(tr_lbl)\n",
    "idx_tensor = tf.constant(idx)\n",
    "\n",
    "tr_img_ds = tf.data.Dataset.from_tensor_slices(tr_img_tensor)\n",
    "tr_lbl_ds = tf.data.Dataset.from_tensor_slices(tr_lbl_tensor)\n",
    "idx_ds = tf.data.Dataset.from_tensor_slices(idx_tensor)\n",
    "\n",
    "tr_ds = tf.data.Dataset.zip((tr_img_ds, tr_lbl_ds)).shuffle(buffer_size=N)\n",
    "\n",
    "tr_ds_i = tf.data.Dataset.zip((tr_ds, idx_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we filter by the index - that's equivalent to splitting the ```Dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ev = tr_ds_i.filter(lambda x,y: y < int(N * ratio))\n",
    "ds_tr = tr_ds_i.filter(lambda x,y: y >= int(N * ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_ev = ds_ev.map(lambda x,y: x).batch(2).make_one_shot_iterator()\n",
    "it_tr = ds_tr.map(lambda x,y: x).repeat(3).batch(8).make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20% of 20 samples means 4. With batch size 2 that's *two* batches, before we run out of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(it_ev.get_next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The other 80% are repeated over three epochs with batch size 8. That's six batches to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(it_tr.get_next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement that in a single utility method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(ds, N, ratio):\n",
    "    idx = np.array(range(N))\n",
    "    idx_ds = tf.data.Dataset.from_tensor_slices(tf.constant(idx))\n",
    "    ds_i = tf.data.Dataset.zip((ds, idx_ds))\n",
    "    ds1 = ds_i.filter(lambda x,y: y < int(N * ratio)).map(lambda x,y: x)\n",
    "    ds2 = ds_i.filter(lambda x,y: y >= int(N * ratio)).map(lambda x,y: x)\n",
    "    return (ds1, ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(tr_img_tensor)\n",
    "ds1, ds2 = split(ds, 20, 0.4)\n",
    "ds1 = ds1.batch(20).make_one_shot_iterator().get_next()\n",
    "ds2 = ds2.batch(20).make_one_shot_iterator().get_next()\n",
    "ds1, ds2 = sess.run([ds1, ds2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is split into 40% / 60% parts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1, ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
