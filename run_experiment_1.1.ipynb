{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import dataset\n",
    "from models.conv2_dense2_dropout import Model\n",
    "#from models.dense3 import Model\n",
    "\n",
    "from helpers.os_utils import os_info\n",
    "from helpers.history import ExpHistory\n",
    "from helpers.estimator_utils import create_model_fn, split_datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to rerun and should close the session, if one is open.\n",
    "try: \n",
    "    sess.close()\n",
    "except NameError:\n",
    "    print(\"Don't worry. Need to ignore this error once\")\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the history and the runtime context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "HIST_FILE_NAME = 'experiment_history.csv'\n",
    "history = ExpHistory(HIST_FILE_NAME)\n",
    "\n",
    "localtime = time.asctime(time.localtime(time.time()))\n",
    "user = os.environ.get('USER', os.environ.get('USERNAME', 'anonymous'))\n",
    "print(\"\\n\\n\")\n",
    "print(\"Welcome, %s, it's %s, and you'll be working with Tensorflow version %s\" % (user, localtime, tf.__version__))\n",
    "rt=os_info()\n",
    "this_os = rt['os']\n",
    "this_node = rt['node']\n",
    "this_machine = rt['machine']\n",
    "this_cuda = rt['cuda']\n",
    "print(\"Your current runtime: \\n  node: %s, \\n  os: %s, \\n  machine: %s, \\n  cuda: %s\" % (this_node, this_os, this_machine, this_cuda))\n",
    "print(\"\\n\")\n",
    "columns=[\n",
    "    'node', \n",
    "    #'os',\n",
    "    #'machine',\n",
    "    'cuda',\n",
    "    'multi_gpu',\n",
    "    'model',\n",
    "    'batch_size',\n",
    "    'data_dir',\n",
    "    #'model_dir',\n",
    "    'train_epochs',\n",
    "    #'user',\n",
    "    #'time_stamp',\n",
    "    'localtime',\n",
    "    'steps',\n",
    "    'accuracy',\n",
    "    'duration'\n",
    "]\n",
    "history.experiments.tail(10)[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want to start with the most recent record from this platform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams=history.suggest_from_history()\n",
    "#hparams=history.copy_from_record(18)\n",
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use as new hyper-parameter record, with adaptations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_SET = 'DIGITS'\n",
    "#hparams.data_dir = '/var/ellie/data/mnist'\n",
    "\n",
    "DATA_SET = 'FASHION'\n",
    "hparams.data_dir = '/var/ellie/data/mnist_fashion'\n",
    "\n",
    "hparams.train_epochs = 2\n",
    "hparams.batch_size = 256\n",
    "hparams.multi_gpu = False\n",
    "hparams.model = Model.id\n",
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always have a quick peek at your input data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dataset.training_dataset(hparams.data_dir, DATA_SET).batch(10).make_one_shot_iterator().get_next()\n",
    "samples = sess.run(samples)\n",
    "f, arr = plt.subplots(2,5)\n",
    "for row in (0, 1):\n",
    "    for col in range(5):\n",
    "        i = 5 * row + col\n",
    "        img = samples[0][i].reshape([28,28])\n",
    "        arr[row, col].imshow(img)\n",
    "samples[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of this tutorial, we always start from scratch\n",
    "!rm -rf /tmp/mnist_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model function constructs the computational graphs for training, eval and test\n",
    "Note that the actual construction takes place within the Estimator. Thus, none of the the constructing code should be explicitly called from the API client. The Estimator will complain that parts that have been constructed prior to those that itself constructs, don't belong to the same graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_function = create_model_fn(\n",
    "    lambda params: Model(params),\n",
    "    tf.train.AdamOptimizer(),\n",
    "    tf.losses.sparse_softmax_cross_entropy,\n",
    "    hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance depends on the data format, and differs between CPU and GPU computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format = ('channels_first' if tf.test.is_built_with_cuda() else 'channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Estimator is the center piece of Tensorflow's new API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier = tf.estimator.Estimator(\n",
    "    model_fn=model_function,\n",
    "    model_dir=hparams.model_dir,\n",
    "    params={\n",
    "        'data_format': data_format,\n",
    "        'multi_gpu': hparams.multi_gpu\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ```input_fn``` functions are a factories for ```DataSet```s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training dataset into training and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    ds_tr = dataset.training_dataset(hparams.data_dir, DATA_SET)\n",
    "    ds_tr_tr, _ = split_datasource(ds_tr, 60000, 0.95)\n",
    "    ds1 = ds_tr_tr.cache().shuffle(buffer_size=57000).\\\n",
    "        repeat(hparams.train_epochs).\\\n",
    "        batch(hparams.batch_size)\n",
    "    return ds1\n",
    "\n",
    "def eval_input_fn():\n",
    "    ds_tr = dataset.training_dataset(hparams.data_dir, DATA_SET)\n",
    "    _, ds_tr_ev = split_datasource(ds_tr, 60000, 0.95)\n",
    "    ds2 = ds_tr_ev.batch(hparams.batch_size)\n",
    "    return ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors_to_log = {'train_accuracy': 'train_accuracy'}\n",
    "logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training and report the new hyper-parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "start_time=time.time()\n",
    "mnist_classifier.train(input_fn=train_input_fn, hooks=[logging_hook])\n",
    "duration=time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "hparams.accuracy = eval_results['accuracy']\n",
    "hparams.steps = eval_results['global_step']\n",
    "hparams.duration = int(duration)\n",
    "\n",
    "# Report!\n",
    "history.report_experiment(hparams)\n",
    "\n",
    "print('Evaluation results:\\n\\t%s' % eval_results)\n",
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
