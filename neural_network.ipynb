{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Super-Simple Neural Network Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some 2-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(x,y):\n",
    "    return (1.3*x-.5)*(1.3*x-.5) + (y-.5)*(y-.5) < .05\n",
    "\n",
    "def createSamples(N, xl, xr, yu, yo, ground_truth, rnd=True):\n",
    "    \"\"\"\n",
    "        ground_truth is a function that calculates the \"true\" label, given coordinates x and y\n",
    "        Produce N samples in the rectangle [xl, xr, yu, yo] with the given ground_truth\n",
    "    \"\"\"\n",
    "    if rnd:\n",
    "        np.random.seed(1234)\n",
    "        x = np.random.uniform(xl,xr,N)\n",
    "        y = np.random.uniform(yu,yo,N)\n",
    "    else:\n",
    "        N = int(math.sqrt(N))\n",
    "        dx = (xr - xl) / N\n",
    "        dy = (yo - yu) / N\n",
    "        field = np.array([(xl + dx * xs,yu + dy * ys) for xs in range(N) for ys in range(N)]).T\n",
    "        x, y = field[0], field[1]\n",
    "\n",
    "    c = ground_truth(x, y) * 1.0\n",
    "    return x, y, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before rerunning, close the previous session. Ignore error the very first time\n",
    "try: \n",
    "    sess.close()\n",
    "except NameError:\n",
    "    print(\"Don't worry. Need to ignore this error once\")\n",
    "sess = tf.InteractiveSession()\n",
    "FLAGS=lambda: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 2-dimensional input data, classes are represented by colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx, sy, sc = createSamples(10000, 0, 1, 0, 1, ground_truth, rnd=False)\n",
    "points=np.array([sx, sy])\n",
    "tr_samples = points.T # Need transposed for use with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (8,8)\n",
    "plt.scatter(sx, sy, c=sc, cmap=\"bwr\", marker='.', s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Network\n",
    "We'll be creating a simple feed-forward network with two hidden layers.\n",
    "\n",
    "![NN](images/NN_2x3x3x2_small.png)\n",
    "\n",
    "Our neural network will be defined as\n",
    "\n",
    "$ f(x) = \\Theta^{(3)} \\cdot \\sigma(\\Theta^{(2)} \\cdot \\sigma(\\Theta^{(1)} \\cdot x + b^{(1)} ) + b^{(2)}) + b^{(3)}$\n",
    "\n",
    "Note, that we omit the final non-linearity at this point. That's for mere technical reasons and doesn't change the story.\n",
    "\n",
    "Below you see the neural network in code, featuring some illustrative initial values.\n",
    "\n",
    "You see: We have 2 input nodes, 3 nodes in each of the hidden layers and again 2 nodes in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight matrices and biases initialized to some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = np.array([[1.6, 4], [1.6, -1.2], [-3.6, 1.6]])              # 3x2 weight Matrix towards the first hidden layer\n",
    "b1 = np.array([[-1, 1, 6]]).T                                        # bias of the first hidden layer\n",
    "Theta2 = np.array([[1, 2, -3], [.5, .2, -3], [2, 1, -.2]])           # 3x3 weight Matrix towards the second hidden layer\n",
    "b2 = np.array([[.2, .1, -.4]]).T                                     # bias of the 2nd hidden layer\n",
    "Theta3 = np.array([[.5, 2, -.03], [.2, 1, -.2]])                     # 2x3 weight Matrix towards the output layer\n",
    "b3 = np.array([[.2, .3]]).T                                          # bias of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# NOTE: You need to initialize with the transpose of the weight matrix, otherwise TF mixes up columns and rows\n",
    "#       Of course that's not a problem, since typically, all numbers are randomly initialized, anyway.\n",
    "# (can you imagine the frustration until I found out?)\n",
    "#\n",
    "W1_i=tf.constant_initializer(Theta1.T)\n",
    "b1_i=tf.constant_initializer(b1)\n",
    "W2_i=tf.constant_initializer(Theta2.T)\n",
    "b2_i=tf.constant_initializer(b2)\n",
    "W3_i=tf.constant_initializer(Theta3.T)\n",
    "b3_i=tf.constant_initializer(b3)\n",
    "\n",
    "def feed_forward(x):\n",
    "    _dense1=tf.layers.Dense(3, activation=tf.nn.sigmoid, kernel_initializer=W1_i, bias_initializer=b1_i)\n",
    "    _dense2=tf.layers.Dense(3, activation=tf.nn.sigmoid, kernel_initializer=W2_i, bias_initializer=b2_i)\n",
    "    _logits=tf.layers.Dense(2, kernel_initializer=W3_i, bias_initializer=b3_i)    \n",
    "\n",
    "    dense1 = _dense1(x)\n",
    "    dense2 = _dense2(dense1)\n",
    "    y = _logits(dense2)\n",
    "    return dense1, dense2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder for the different data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype=tf.float64, shape=[None, 2])\n",
    "L = tf.placeholder(dtype=tf.int64, shape=[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the computational graph for the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = tf.one_hot(L, depth=2)\n",
    "hidden1, hidden2, output = feed_forward(X)\n",
    "probs = tf.nn.softmax(output)\n",
    "objective = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "train = optimizer.minimize(objective)\n",
    "preds = tf.argmax(probs,axis=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, L), dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "losses = []\n",
    "accies = []\n",
    "n_batch = 50\n",
    "\n",
    "def single_batch(n_batch):\n",
    "    for _ in range(n_batch):\n",
    "        _, _all_output, _objective, _accuracy = sess.run([train, output, objective, accuracy], feed_dict={X: tr_samples, L: sc.astype(int)})\n",
    "\n",
    "    print (\"Loss: %s - Accuracy: %s\" % (_objective, _accuracy))\n",
    "    losses.append(_objective)\n",
    "    accies.append(_accuracy)\n",
    "    return _all_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's look at the hidden layers before the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, h1, h2 = sess.run([preds, hidden1, hidden2], feed_dict={X: tr_samples, L: sc.astype(int)})\n",
    "h1 = h1.T\n",
    "h2 = h2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first hidden layer before the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (12,12)\n",
    "plt.axes(projection='3d', elev=30, azim=110)\n",
    "plt.scatter(*h1, c=sc, cmap='bwr', marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Second Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axes(projection='3d', elev=40, azim=240)\n",
    "plt.scatter(*h2, c=sc, cmap='bwr', marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do 50 Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range (25): \n",
    "    all_output = single_batch(n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to Linearly Separate\n",
    "The network learned to tweak the manifold of the last hidden layer such that a hyperplane can separate red from blue points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, h1, h2 = sess.run([preds, hidden1, hidden2], feed_dict={X: tr_samples, L: sc.astype(int)})\n",
    "h1 = h1.T\n",
    "h2 = h2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The First Hidden Layer After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axes(projection='3d', elev=20, azim=70)\n",
    "plt.scatter(*h1, c=sc, cmap='bwr', marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Second Hidden Layer After the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axes(projection='3d', elev=40, azim=240)\n",
    "plt.scatter(*h2, c=sc, cmap='bwr', marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After some period of stagnation, the network learned to predict the colors from the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accies)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring the classes (colors) of some given test coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=20\n",
    "sx, sy, sc = createSamples(N, 0, 1, 0, 1, ground_truth)\n",
    "points=np.array([sx, sy])\n",
    "plt.scatter(sx, sy, c=sc, cmap=\"bwr\", marker='.')\n",
    "\n",
    "test_samples = np.array([sx, sy]).T\n",
    "test_labels = sc.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network correctly infers (most of) the classes of the given test coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_infered = sess.run(preds, feed_dict={X: test_samples, L: test_labels})\n",
    "\n",
    "print(\"true classes   : %s\" % test_labels)\n",
    "print('infered classes: %s' % test_infered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
